####################GAUSSIAN MIXTURE MODEL##########################################
import numpy as np
import pandas as pd
import pylab as plt    
from matplotlib.patches import Ellipse
    
class GMM:
    def __init__(self, k , epsilon = 0.0001):
        self.k = k 
        # Stopping condition
        self.epsilon = epsilon 
    
    def EM(self, X, max_iterations = 200):
        mu = X[np.random.choice(n, self.k, False), :]
        n, d = X.shape
        
        # initialize the covariance matrices for each Gaussian and probabilities for each Gaussian
        # Initialize responsibility matrix to zeros
        Sigma= [np.eye(d)] * self.k
        w = [1./self.k] * self.k        
        R = np.zeros((n, self.k))
        
        # log likelihoods
        log_likelihoods = []
        P = lambda mu, s: np.linalg.det(s) ** -.5 ** (2 * np.pi) ** (-X.shape[1]/2.) \
                * np.exp(-.5 * np.einsum('ij, ij -> i',\
                        X - mu, np.dot(np.linalg.inv(s) , (X - mu).T).T ) )
                        
        # iterate algorithm until stop condition 
        while len(log_likelihoods) < max_iterations:
            
            # EXPECTATION
            # calculate the membership for each of k -gaussians
            for k in range(self.k):
                R[:, k] = w[k] * P(mu[k], Sigma[k])

            # Likelihood computation
            log_likelihood = np.sum(np.log(np.sum(R, axis = 1)))
            log_likelihoods.append(log_likelihood)
           
            # Normalize responsibility matrix
            R = (R.T / np.sum(R, axis = 1)).T
            N_ks = np.sum(R, axis = 0)
            
            # MAXIMIZATION
            # new mean and covariance for each Gaussian using responsibility
            for k in range(self.k):
                #means, covariance and probabilities
                mu[k] = 1. / N_ks[k] * np.sum(R[:, k] * X.T, axis = 1).T
                x_mean = np.matrix(X - mu[k])
                Sigma[k] = np.array(1 / N_ks[k] * np.dot(np.multiply(x_mean.T,  R[:, k]), x_mean))
                w[k] = 1. / n * N_ks[k]

            # check convergence
            if len(log_likelihoods) < 2 : continue
            if np.abs(log_likelihood - log_likelihoods[-2]) < self.epsilon: break
        
        # assign values
        from collections import namedtuple
        self.param = namedtuple('param', ['mu', 'Sigma', 'w', 'log_likelihoods', 'n_iters'])
        self.param.mu = mu
        self.param.Sigma = Sigma
        self.param.w = w
        self.param.log_likelihoods = log_likelihoods
        self.param.n_iters = len(log_likelihoods)
        return self.param
    
    def plot_log_likelihood(self):
        plt.plot(self.param.log_likelihoods)
        plt.title('Log Likelihood by Iteration')
        plt.xlabel('Iterations')
        plt.ylabel('log likelihood')
        plt.show()
    
    def pred(self, x):
        p = lambda mu, s : np.linalg.det(s) ** - 0.5 * (2 * np.pi) **\
                (-len(x)/2) * np.exp( -0.5 * np.dot(x - mu , \
                        np.dot(np.linalg.inv(s) , x - mu)))
        probs = np.array([w * p(mu, s) for mu, s, w in \
            zip(self.param.mu, self.param.Sigma, self.param.w)])
        return probs/np.sum(probs)       

if __name__ == "__main__":
    def plot_ellipse(pos, cov, nstd=2, ax=None, **kwargs):
        def eigsorted(cov):
            vals, vecs = np.linalg.eigh(cov)
            order = vals.argsort()[::-1]
            return vals[order], vecs[:,order]
    
        if ax is None:
            ax = plt.gca()
    
        vals, vecs = eigsorted(cov)
        theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))
    
        #Width and height are "full" widths, not radius
        width, height = 2 * nstd * np.sqrt(abs(vals))
        ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwargs)
    
        ax.add_artist(ellip)
        return ellip    
    
    def show(X, mu, cov):
        plt.cla()
        K = len(mu) # number of clusters
        colors = ['b', 'k', 'g', 'c', 'm', 'y', 'r']
        plt.plot(X.T[0], X.T[1], 'm*')
        for k in range(K):
          plot_ellipse(mu[k], cov[k],  alpha=0.6, color = colors[k % len(colors)])

# Load data  
    # Load data  
    df = pd.read_csv('GMM_dataset.txt', sep="  ", header=None)
    dataset = df.as_matrix()
    #choose hyper parameters
    k = 5
    epsilon = .00001
    max_iterations = 200
    r = 10
    
    #fit data r times
    for i in range(r):
        gmm = GMM(k, epsilon)
        param = gmm.EM(dataset, max_iterations)
        print("log likelihood:", param.log_likelihoods[-1])
        print("mu: ", param.mu)
        print("sigma: ",param.Sigma)
        fig = plt.figure(figsize = (12, 6))
        fig.add_subplot(121)
        show(dataset, param.mu, param.Sigma)
        fig.add_subplot(122)
        plt.plot(np.array(param.log_likelihoods))
        plt.title('Log Likelihood by Iteration')
        plt.xlabel('Iterations')
        plt.ylabel('log likelihood')
        plt.show()
